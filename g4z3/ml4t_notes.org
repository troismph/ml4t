#+TITLE:  ml4t Notes

* Noisy Signals

- Kalman filter
  - KF's "hidden state" and "observation" are all random vectors, or multivariate random variables. see wikipedia
- Wavelets

* Evaluations

- Alpha: the average return in excess of a benchmark
- tools
  - Zipline: backtest pipeline
  - Alphalens: multi-facet measurements on factors
- Evaluations (of factor)
  - IR: information ratio, alpha divided by the tracking risk (of the benchmark)
    - \( \frac{\alpha}{\sigma_{\alpha}} \)
    - when the benchmark is the risk-free rate, IR is the Sharpe ratio
  - IC: information coefficient, correlation between predicted and actual returns
    - Spearman rank correlation between predicted and actual returns
- Portfolio management
  - Kelly criterion: [[https://en.wikipedia.org/wiki/Kelly_criterion][wikipedia]]
    - how to size a bet
  - Diversification is protection against ignorance. It makes little sense if you know what you are doing. - Warren Buffet
  - the fundamental law of active management:
    - \( IR \approx IC * \sqrt{breadth} \)
    - play well (IC) and play a lot (breadth)
  - Mean-variance portfolio optimization
    - expected return vs. risk (variance)
    - efficient frontier
    - various methods to optimize
      - vs. the naive 1/N portfolio
    - when to use: your algorithm outputs a vector of weights, now optimize these weights with above methods to improve
  - pyfolio: portfolio and risk analytics in python
    - Alphalens is for factors


* ML in trading

- basic walk-through
  - prediction vs. inference
    - prediction: the "inference" as in AI
    - inference: explanation how the outcomes (maybe governed by prediction) are generated
      - statistical association of variables (in the context of prediction, also includes the association between the input and output)a

* Linear Models

- Heteroskedasticity & Homoskedasticity:
  - definition of heteroskedasticity: the variance of the error term is not constant
  - [[file:Linear_Models/2023-09-27_14-12-27_screenshot.png]]


* Misc
** Concepts

- Lagged return: returns from previous periods as input variables or features to compute new values (e.g. feature, model)

** Tools
- TA-Lib: technical analysis lib. originally written in C++ , popularized by a python binding. see https://github.com/TA-Lib/ta-lib-python
- Seaborn: data visualization toolkit
- Yellowbrick: extension on scikit-learn for visualization, especially in cross validation & model selection


** Quotes

- Nassim Taleb: life is long gamma


* Maybe TODO

- play with data from [[https://blog.curtii.com/blog/posts/wordscapes-the-casino-in-your-pocket/]["the casino in your pocket"]]
  - [[file:~/src/ml4t/data/mt-fortune.xlsx][data file]]


* Journal

- [2023-09-26 Tue]
  - basic workflow of a machine learning task
    - problem definition: regression, classification, clustering, etc.
      - for algorithmic trading, it's usually regression
    - preparing data
    - feature engineering
      - finding the most influential features
      - pre-processing features (e.g. normalization)
    - model selection
      - cross validation
    - hyper-parameter tuning
      - hyper-parameters are parameters that cannot be learned from the data
      - grid search
    - post-training analysis
      - learning curve
  - concepts
    - mutual information: finding the most influential features
  - tools
    - yellowbricks
- [2023-09-27 Wed]
  - linear models
    - OLS: ordinary least squares
    - stochastic gradient descent
  - concepts
    - heteroskedasticity
    - homoskedasticity
  - tools
    - statsmodels
  - understanding linear model summary
    - data characteristics
    - model characteristics: R-squared, F-statistic, P-value
    - residual analysis: Omnibus, Skew, Kurtosis, Durbin-Watson, Jarque-Bera, Condition Number
  - Fama-Macbeth regression: a method to create linear model based on Fama-French 5 factors
    - with ready-to-use open source implementation
      - from linearmodels.asset_pricing import LinearFactorModel
    - the notebook [[file:~/src/ml4t/07_linear_models/02_fama_macbeth.ipynb][02_fama_macbeth.ipynb]] spends a lot of time explaining Fama-Macbeth regression, and actually we can simply use the above open source implementation
